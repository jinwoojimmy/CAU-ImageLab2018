# 04/07/18 Daily Report

## PGM(Probabilistic Graphical Model)

### Why we should learn?
  
    To deal with the uncertainty in reality by modeling the world in the form of a probability distribution.   

### Intro
  
  Probabilistic Graphical models are a marriage between graph theory and probability theory.
  They clarify the relationship between neural networks and related network-based models such asHMMs, MRFs, and Kalman filters.
  Probabilistic modeling is widely used throughout machine learning and in many real-world applications. 
  These techniques can be used to solve problems in fields as diverse as medicine, language processing, vision, and many others.

### Key idea
  - Represent the world as a collection of random variables *X1, . . . , Xn* with joint distribution *p(X1, . . . , Xn)*
  - Learn the distribution from data
  - Perform “inference” (compute conditional distributions *p(Xi| X1 = x1, . . . , Xm = xm)*)


### Advantages of the graphical model point of view
  - Incorporation of domain knowledge and causal (logical) structures
  - Inference and learning are treated together
  - Supervised and unsupervised learning are merged seamlessly
  - Missing data handled nicely
  - A focus on conditional independence and computational issues
  - Interpretability (if desired)
  
### Two main kinds of PGM
  - Directed graphical model(DGM)
  - Undirected graphical model(UGM)
  
### Learning and Inference
  - It is not necessary to learn that which can be inferred
  - The weights in a network make local assertions about the relationships between neighboring nodes
  - Inference algorithms turn these local assertions into global assertions about the relationships between nodes 
    - e.x) the probability of an input vector given an output vector
  - This is achieved by associating a joint probability distribution with the network

### Two important rules
  - **Chain rule**
    Let S1, . . . Sn be events, p(Si) > 0
    
    ![equation](https://latex.codecogs.com/gif.latex?p%28S_%7B1%7D%20%5Ccap%20S_%7B2%7D%20%5Ccap%2C%20...%2C%5Ccap%20S_%7Bn%7D%29%20%3D%20p%28S_%7B1%7D%29p%28S_%7B2%7D%20%7C%20S_%7B1%7D%29...%20p%28S_%7Bn%7D%20%7C%20S_%7B1%7D%2C%20.%20.%20.%20%2C%20S_%7Bn-1%7D%29)
  - **Bayes’ rule** 
    Let C, x be events, p(C) > 0 and p(x) > 0
    
    ![equation](https://latex.codecogs.com/gif.latex?P%28C%7Cx%29%20%3D%20%5Cfrac%7BP%28C%29P%28x%7CC%29%7D%7BP%28x%29%7D)


### Representation of PGM
  
<img src="https://github.com/jwcse/DeepLearning/blob/master/img/pgm_representation.png" width="700" height="500">



#### Reference :
[Stanford CS 228: Probabilistic Graphical Models - preliminaries/introduction](https://ermongroup.github.io/cs228-notes/preliminaries/introduction/)

[Introduction to Graphical Models by Michael I. Jordan](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.7467&rep=rep1&type=pdf)

[Medium posting by Neeraj Sharma : Understanding Probabilistic Graphical Models Intuitively](https://medium.com/@neerajsharma_28983/intuitive-guide-to-probability-graphical-models-be81150da7a)
