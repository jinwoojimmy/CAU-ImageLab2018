# 04/08/18 Daily Report

## Stacked Autoencoder (SAE)

As we can extract various features using many hidden layers, autoencoder can be implemented using many hidden layers.

This structure is called **Stacked Autoencoder**.

<img src="https://github.com/jwcse/DeepLearning/blob/master/img/stacked_autoencoder.png" width="500" height="500">


On above picture, we can figure out that *feature 1* is computed by given input, and *feature 2* is computed by *feature 1*.

We can figure out that each hidden layer shows more compact representation compared with prior layer.

Basically, this is stacked structure of autoencoder, and training is proceeded by **"Greedy Layer-Wise Training"**.

### Greedy Layer-Wise Training

Before 2006, researchers had difficulty on training network which has more than two hidden-layers.

As 2006 has come, three famous researchers found out how to train deep layers of network. The following papers provided solution.

  * Hinton - A Fast Learning Algorithm for Deep Belief Nets
  
  * Bengio - Greedy Layer - Wise Training for Deep Networks
  
  * LeCun - Efficient Learning of Sparse Representations with an Energy-Based Model
  
By these people's research, difficulty of training deep layered network had been solved. The problem was *vanishing gradient* and *overfitting*.



One of solution is  **Greedy Layer-Wise Training**. The concept is shown on the below picture.

<img src="https://github.com/jwcse/DeepLearning/blob/master/img/greedy_layer_wise_training.PNG" width="500" height="400">


As the term says, training is performed by layer in greedy way. Each layer is trained, assuming there is no layer beyond the present layer.

In addition, prior layer's parameter is fixed and present layer's parameter is updated.


#### Unsupervised Pre-training

The **Greedy Layer-Wise Training** can be used not only for Stacked AE, but also for network of supervised learning or CNN.

When training data exists enough, but labeled data is not abundant, pre-training can be done using unlabeled training data. This is called
**unsupervised pre-training**.

This kind of training way has been used since 2006. However after 2010, as activation function *RELU* appeared and *dropout*,
*maxout*, *data augmentation* and *batch-normalization* has been published, this way is rarely used now. Because just using supervised learning, it has been possible to get good performance.





