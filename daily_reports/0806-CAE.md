# 06/08/18 Daily Report

## Convolutional Autoencoder(CAE)

### Intro

The concept of Convolutional Autoencoder was first published on 2011 by Jonathan Masci from Swiss, with name of 
[Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction](http://pdfs.semanticscholar.org/1c6d/990c80e60aa0b0059415444cdf94b3574f0f.pdf)

After that good papers are being published. 

Here is one interesting paper, [DeepPainter: Painter Classification Using Deep Convolutional Autoencoders](http://elidavid.com/pubs/deeppainter.pdf) which was published at 2016.



Usually, a painter's artifact is not that much and it is difficult to utilize *data augmentation* to increase training data. 

Therefore, training with small quantity of training data in supervised learning way can lead to overfitting problem.

So we can consider unsupervised learning like using Autoencoder.



### DeepPainter

The DeepPainter trains with unsupervised learning. In order to do so, fully connected layer is seperated and convolutional layer and pooling layer part is trained with Autoencoder-way.

After training, fully connected layer part is fine-tuned in supervised learning-way.

So, front part is form of Stacked Convolutional Autoencoder and Decoder part should be added in order to train.


<img src="https://github.com/jwcse/DeepLearning/blob/master/img/CAE_overview.PNG" width="750" height="200">

In DeepPainter, max-pooling is used but this can be problem when constitute decode network, because the extracted maximum value in the pooling window can't be placed in appropriate position when size is shrinked.

So, in DeepPainter, pooling position is saved, so that we can figure out appropriate position when unpooling.

The picture below shows the way.

<img src="https://github.com/jwcse/DeepLearning/blob/master/img/CAE_unpooling.PNG" width="500" height="400">



After this process, training is proceeded as traditional training way of Autoencoder, which means that specific labeling is not needed because it is proceeded in unsupervised learning-way.

In unsupervised learning, training data was noised as case of Denoising Autoencoder.

After finish of training, decoder is eliminated and fully connected layer for classification is connected.


Now, classifier part is trained. Because front part is already trained, this can be categorized as fine-tuning.




#### Performance Result of DeepPainter

<img src="https://github.com/jwcse/DeepLearning/blob/master/img/CAE_performance.PNG" width="600" height="500">


### Example Code

Referenced from [GunhoChoi's code](https://github.com/GunhoChoi/PyTorch-FastCampus/blob/master/08_Autoencoder/1_Convolutional_Autoencoder.ipynb)

```python



```


