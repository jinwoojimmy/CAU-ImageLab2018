# 28/05/18 Daily Report


## CNN Architecture

*CNN(Convolutional Neural Network)* is neural network utilized in image processing , MLP and so on.
The key idea is *convolution*, which means only small portion of image is handled at a time. This small portion is called 'patch',
and whole input image is processed by size of the patch.

CNN is classifed as locally connected nerual net, and this kind of neural net produces smaller weights compared to fully connected neural net. 
And this enables flexible handling of input image data.

1. Convolutional Layer - creation of feature map

Within the size of patch, same size of filter(kernel) which contains weights are computed to input image. The computation is done by 
dot product. After computation, window is moved and computation is perfomed again. Each result of computation value is put into element of
feature map. The window is moved by step, called 'stride'. Also, 'padding' can be done to the original image, which means zero values can be added
to boundary of the input value array.
The output size should be different based on stride and patch size. For example, let's assume that input image size is 32x32x1,
filter size is 5x5x1, and stride is 1x1. Then, output array size would be 28x28.

In addition, depending on number of filters, output's depth would be different. Each filter's size is same but contains different values.
If we use 6 filters, for instance, the output's depth would be 6.

2. Pooling Layer - subsample

Purpose is to reduce information generated by convolutional layer.
One of the representative way is *max pooling*. With certain filter size and stride, maximum value in the area is put into the element of output.

3. Feedforward layer - classification ; fully connected layer

This utilizes features produced by convolution layer and pooling layer to classify.


### Snippet Code in Pytorch

```python
    # convolution layer
    torch.nn.Conv2d(in_channels, out_channels, kernel_size)
    
    # poolying layer
    nn.MaxPool2d(kernel_size)
    
    # Feedforward layer
    nn.Linear(320, 10)  # and then return with softmax(for example)  
```

### Convolution 1x1 - Solve problems in deeper neural net
As neural net goes deeper, it seems that loss goes down. However, quantity of computation increase as more layers exist.

We can solve this by using *inception* concept.

Inception module is utilizes various filters in computing convolution and concatenates those each other.

But for the concept, dimension reduction is conducted by 1x1 Conv. By doing so, quantity of computation decrease.





